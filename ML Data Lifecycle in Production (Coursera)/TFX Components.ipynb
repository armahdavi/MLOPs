{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All modeuls\n",
    "\n",
    "## generic\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.python.lib.io import file_io # for schema freezing\n",
    "from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList, DatasetFeatureStatistics\n",
    "# DatasetFeatureStatisticsList is to make the type of DatasetFeatureStatisticsList for visualize_statistics\n",
    "import tempfile\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "\n",
    "## Tensorflow Extended Overall\n",
    "from tfx import v1 as tfx\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "\n",
    "## TF Data validation\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_data_validation.utils import slicing_util # to split the data for further representativeness verifications\n",
    "\n",
    "\n",
    "## TF Transform\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from util import add_extra_rows\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbf3c5",
   "metadata": {},
   "source": [
    "![title](TFX_comp11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "### Data Validation not belonging to StatisticsGen SchemaGen and Example Validator ###\n",
    "######################################################################################\n",
    "\n",
    "### Subject 1: to create statistics (descriptive) from a dataframe\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(train_df) # or eval_df or any dataframe (pandas)\n",
    "# train_stats has the DatasetFeatureStatisticsList type (required for visualize_statistics coming in the next line)\n",
    "\n",
    "## if you have only one set\n",
    "tfdv.visualize_statistics(train_stats)\n",
    "## if you have two sets to compare\n",
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=eval_stats, \n",
    "    rhs_statistics=train_stats, \n",
    "    lhs_name='EVAL_DATASET', \n",
    "    rhs_name='TRAIN_DATASET'\n",
    ")\n",
    "\n",
    "### Subject 2: To create and display schema from a df_stats\n",
    "\n",
    "tfdv.infer_schema(statistics = train_stats)\n",
    "tfdv.display_schema(schema)\n",
    "\n",
    "\n",
    "### Subject 3: To capture (and display) anolamies (outliers) (done by validate_statistics)\n",
    "\n",
    "anomalies = tfdv.validate_statistics(statistics = eval_stats, schema = schema) # eval_stats makes sense since schema is defined for train_stats\n",
    "tfdv.display_anomalies(anomalies)\n",
    "\n",
    "## If you want to change minimum fraction percentage to a value lower than the default 1 %\n",
    "example_feature_name = tfdv.get_feature(schema, 'example_feature_name')\n",
    "example_feature_name.distribution_constraints.min_domain_mass = 0.9 # any value between 0 and 1\n",
    "# This is good when you have multiple categories with small fractions\n",
    "\n",
    "## If you want to include a new category that is captured as anomaly due to low fraction\n",
    "example_feature_name = tfdv.get_domain(schema, 'example_feature_name')\n",
    "race_domain.value.append('Example_category')\n",
    "\n",
    "## If we want to set the range for a continous variable]\n",
    "tfdv.set_domain(schema, 'cont_var', schema_pb2.IntDomain(name = 'cont_var', min = 17, max = 90)) # 17 and 90 considered as minimum and maximum examples\n",
    "\n",
    "## See schema after all updates (updates performed inplace) and check anomalies\n",
    "tfdv.display_schema(schema)\n",
    "new_anomalies = tfdv.validate_statistics(statistics = eval_stats, schema = schema)\n",
    "tfdv.display_anomalies(new_anomalies)\n",
    "\n",
    "### Subject 4: Splitting the data per category to verify the representativeness (e.g., sex)\n",
    "\n",
    "slice_fn = slicing_util.get_feature_value_slicer(features = {'sex': None}) # making a slicing function. What if we change None?\n",
    "# need to make a stats options to make the above line workable\n",
    "slice_stats_options = tfdv.StatsOptions(schema = schema,\n",
    "                                           slice_functions = [slice_fn],\n",
    "                                           infer_type_from_schema = True) \n",
    "\n",
    "# If we want to pring all the fueatures\n",
    "for feature in stats_options.feature_allowlist:\n",
    "    print(feature)\n",
    "\n",
    "# This can also have feature_allowlist with a list of columns to be included in the model (see the Assignment)\n",
    "train_df.to_csv(CSV_PATH) # CSV_PATH is an example of teh csv file required for the line below\n",
    "sliced_stats = tfdv.generate_statistics_from_csv(CSV_PATH, stats_options = slice_stats_options) # requires a CSV file \n",
    "# sliced_stats and train_stats (at the beginning of the code) have the same type (DatasetFeatureStatisticsList)\n",
    "# sliced_stats has different components the most famous of which is datasets which is like a dictionary\n",
    "# this dictionary values are like dictionaries as well (See Lab)\n",
    "# for example the following line gives the name of slices.\n",
    "[sliced.name for sliced in sliced_stats.datasets]\n",
    "\n",
    "## To visualize for sliced datasets\n",
    "male_stats_list = DatasetFeatureStatisticsList() # creates a type of DatasetFeatureStatisticsList required for visualize_statistics\n",
    "male_stats_list.datasets.extend([sliced_stats.datasets[1]]) # fill it with sliced datasets\n",
    "male_stats_name = sliced_stats.datasets[1].name\n",
    "\n",
    "female_stats_list = DatasetFeatureStatisticsList()\n",
    "female_stats_list.datasets.extend([sliced_stats.datasets[2]])\n",
    "demale_stats_name = sliced_stats.datasets[2].name\n",
    "\n",
    "tfdv.visualize_statistics(\n",
    "    ils_statistics = male_stats_list,\n",
    "    rhs_statistics = female_stats_list,\n",
    "    lhs_name = male_stats_name,\n",
    "    rhs_name = female_stats_name\n",
    "    )\n",
    "\n",
    "\n",
    "### Subject 5: Skewness & Drift Capturing in datasets\n",
    "\n",
    "# to calculate skew and drift from a feature\n",
    "example_feature = tfdv.get_feature(schema, 'example_feature')\n",
    "example_feature.skew_comparator.infinity_norm.threshold = 0.03\n",
    "example_feature.drift_comparator.infinity_norm.threshold = 0.03\n",
    "\n",
    "# to calculate (and display) anomalies based on skew and drift\n",
    "skew_drift_anomalies = tfdv.validate_statistics(train_stats, schema,\n",
    "                                          previous_statistics=eval_stats,\n",
    "                                          serving_statistics=serving_stats)\n",
    "\n",
    "tfdv.display_anomalies(skew_drift_anomalies)\n",
    "\n",
    "\n",
    "### Sunject 6: Schema Freezing\n",
    "OUTPUT_DIR = 'output'\n",
    "file_io.recursive_create_dir(OUTPUP_DIR)\n",
    "schema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')\n",
    "tfdv.write_schema_text(schema, schema_file) # two arguments: schema (you know it), and the file which store schema (schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4229ceb",
   "metadata": {},
   "source": [
    "![title](TFX_comp_transform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c16489",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### Tensorflow Transform ###\n",
    "############################\n",
    "\n",
    "### Subject 1: Defining a schema using DatasetMetadata\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "        's': tf.io.FixedLenFeature([], tf.string),\n",
    "    }))\n",
    "\n",
    "\n",
    "### Subject 2: To create a preprocessing function that transform/scale raw data\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    \n",
    "    # extract the columns and assign to local variables\n",
    "    x = inputs['x']\n",
    "    y = inputs['y']\n",
    "    s = inputs['s']\n",
    "    \n",
    "    # data transformations using tft functions\n",
    "    x_centered = x - tft.mean(x)\n",
    "    y_normalized = tft.scale_to_0_1(y)\n",
    "    s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "    x_centered_times_y_normalized = (x_centered * y_normalized)\n",
    "    \n",
    "    # return the transformed data\n",
    "    return {\n",
    "        'x_centered': x_centered,\n",
    "        'y_normalized': y_normalized,\n",
    "        's_integerized': s_integerized,\n",
    "        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "    }\n",
    "\n",
    "## Note the input data should have the form of the follwin g format: a list of dictionaries (for input data, or DatasetMetadata for schema)\n",
    "\n",
    "raw_data = [\n",
    "      {'x': 1, 'y': 1, 's': 'hello'},\n",
    "      {'x': 2, 'y': 2, 's': 'world'},\n",
    "      {'x': 3, 'y': 3, 's': 'hello'}\n",
    "  ]\n",
    "\n",
    "# DatasetMetadata looks like a dictionary of dictionaries. The only key is 'schema' and the value is the features with their schema info\n",
    "# this is how it looks like:\n",
    "\n",
    "'''\n",
    "{'_schema': feature {\n",
    "  name: \"s\"\n",
    "  type: BYTES\n",
    "  presence {\n",
    "    min_fraction: 1.0\n",
    "  }\n",
    "  shape {\n",
    "  }\n",
    "}\n",
    "feature {\n",
    "  name: \"x\"\n",
    "  type: FLOAT\n",
    "  presence {\n",
    "    min_fraction: 1.0\n",
    "  }\n",
    "  shape {\n",
    "  }\n",
    "}\n",
    "feature {\n",
    "  name: \"y\"\n",
    "  type: FLOAT\n",
    "  presence {\n",
    "    min_fraction: 1.0\n",
    "  }\n",
    "  shape {\n",
    "  }\n",
    "}\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "### Subject 3: Implementing the the preprocessing function to the raw_data\n",
    "\n",
    "## To ignore the warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "## To implement the preprocessing function to raw_data\n",
    "with tft_beam.Context(temp_dir = tempfile.mkdtemp()):\n",
    "    \n",
    "    # define the pipeline using Apache Beam syntax\n",
    "    transformed_dataset, transform_fn = (\n",
    "        \n",
    "        # analyze and transform the dataset using the preprocessing function\n",
    "        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn)\n",
    "    )\n",
    "\n",
    "# unpack the transformed dataset\n",
    "transformed_data, transformed_metadata = transformed_dataset # the transformed data has a format of list (of dictionaries) like raw_data\n",
    "\n",
    "## Each of transformed_datasets and transform_fn have two components returned by the AnalyzeAndTransformDataset using preprocessing_fn\n",
    "## It does the work of both AnalyzeDataset and TransformDataset (it is a combination performer) each return one value\n",
    "\n",
    "\n",
    "\n",
    "### Subject 4: Another way to use TFT (afterr SchemaGen, StatsitcsGen and Example Validator). See the next two cells\n",
    "\n",
    "# Set the constants module filename\n",
    "_census_constants_module_file = 'census_constants.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52691c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_census_constants_module_file}\n",
    "\n",
    "# Features with string data types that will be converted to indices\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'education', 'marital-status', 'occupation', 'race', 'relationship', 'workclass', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "# Numerical features that are marked as continuous\n",
    "NUMERIC_FEATURE_KEYS = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# Feature that can be grouped into buckets\n",
    "BUCKET_FEATURE_KEYS = ['age']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each bucket feature.\n",
    "FEATURE_BUCKET_COUNT = {'age': 4}\n",
    "\n",
    "# Feature that the model will predict\n",
    "LABEL_KEY = 'label'\n",
    "\n",
    "# Utility function for renaming the feature\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transform module filename\n",
    "_census_transform_module_file = 'census_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_census_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import census_constants\n",
    "\n",
    "# Unpack the contents of the constants module\n",
    "_NUMERIC_FEATURE_KEYS = census_constants.NUMERIC_FEATURE_KEYS\n",
    "_CATEGORICAL_FEATURE_KEYS = census_constants.CATEGORICAL_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_KEYS = census_constants.BUCKET_FEATURE_KEYS\n",
    "_FEATURE_BUCKET_COUNT = census_constants.FEATURE_BUCKET_COUNT\n",
    "_LABEL_KEY = census_constants.LABEL_KEY\n",
    "_transformed_name = census_constants.transformed_name\n",
    "\n",
    "\n",
    "# Define the transformations\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "        inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "        Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    # Scale these features to the range [0,1]\n",
    "    for key in _NUMERIC_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.scale_to_0_1(\n",
    "            inputs[key])\n",
    "    \n",
    "    # Bucketize these features\n",
    "    for key in _BUCKET_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.bucketize(\n",
    "            inputs[key], _FEATURE_BUCKET_COUNT[key])\n",
    "\n",
    "    # Convert strings to indices in a vocabulary\n",
    "    for key in _CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(inputs[key])\n",
    "\n",
    "    # Convert the label strings to an index\n",
    "    outputs[_transformed_name(_LABEL_KEY)] = tft.compute_and_apply_vocabulary(inputs[_LABEL_KEY])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore TF warning messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Instantiate the Transform component\n",
    "transform = tfx.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(_census_transform_module_file))\n",
    "\n",
    "# Run the component\n",
    "context.run(transform)\n",
    "\n",
    "\n",
    "# Get the uri of the transform graph\n",
    "transform_graph_uri = transform.outputs['transform_graph'].get()[0].uri\n",
    "\n",
    "# List the subdirectories under the uri\n",
    "os.listdir(transform_graph_uri)\n",
    "\n",
    "# Get the URI of the output artifact representing the transformed examples\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Get 3 records from the dataset\n",
    "sample_records_xf = get_records(transformed_dataset, 3)\n",
    "\n",
    "# Print the output\n",
    "pp.pprint(sample_records_xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5f048",
   "metadata": {},
   "source": [
    "![title](TFX_comp21.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb779cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "### Tensorflow Extended (TFX) (other then specific components) ###\n",
    "##################################################################\n",
    "\n",
    "# Initialize the InteractiveContext with a local sqlite file.\n",
    "# If you leave `_pipeline_root` blank, then the db will be created in a temporary directory.\n",
    "# You can safely ignore the warning about the missing config file.\n",
    "\n",
    "# location of the pipeline metadata store\n",
    "_pipeline_root = './pipeline/'\n",
    "\n",
    "context = InteractiveContext(pipeline_root = _pipeline_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbb735",
   "metadata": {},
   "source": [
    "![title](TFX_comp_ExampleGen.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### ExampleGen from Data Ingestion ###\n",
    "######################################\n",
    "\n",
    "### Subject 1: To instantiate ExampleGen with the input CSV dataset\n",
    "\n",
    "# directory of the raw data files\n",
    "_data_root = './data/census_data'\n",
    "\n",
    "# path to the raw training data\n",
    "_data_filepath = os.path.join(_data_root, 'adult.data')\n",
    "\n",
    "example_gen = tfx.components.CsvExampleGen(input_base = _data_root)\n",
    "context.run(example_gen) # the outcome is an artifact\n",
    "\n",
    "# to get artifact object and read from its properties\n",
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "print(f'split names: {artifact.split_names}')\n",
    "print(f'artifact uri: {artifact.uri}')\n",
    "\n",
    "\n",
    "### Subject 2: To collect all the data\n",
    "## Get the URI of the output artifact representing the training examples\n",
    "train_uri = os.path.join(artifact.uri, 'Split-train')\n",
    "\n",
    "## See the contents of the `train` folder\n",
    "!ls {train_uri}\n",
    "\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)] #makes only one element list if run from the lab\n",
    "\n",
    "## Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type = \"GZIP\")\n",
    "\n",
    "# dataset has data. It is just not as visible as a simple dataframe. Run the following code, then you will realize\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "each record  of dataset looks like this (this is the first record)\n",
    "\n",
    "\n",
    "{'features': \n",
    " {'feature': \n",
    "  {'education': \n",
    "   {'bytesList': {'value': ['IEJhY2hlbG9ycw==']}}, \n",
    "   'sex': {'bytesList': {'value': ['IE1hbGU=']}}, \n",
    "   'race': {'bytesList': {'value': ['IFdoaXRl']}}, \n",
    "   'native-country': {'bytesList': {'value': ['IFVuaXRlZC1TdGF0ZXM=']}}, \n",
    "   'relationship': {'bytesList': {'value': ['IE5vdC1pbi1mYW1pbHk=']}}, \n",
    "   'marital-status': {'bytesList': {'value': ['IE5ldmVyLW1hcnJpZWQ=']}}, \n",
    "   'hours-per-week': {'int64List': {'value': ['40']}}, \n",
    "   'age': {'int64List': {'value': ['39']}}, \n",
    "   'label': {'bytesList': {'value': ['IDw9NTBL']}}, \n",
    "   'occupation': {'bytesList': {'value': ['IEFkbS1jbGVyaWNhbA==']}}, \n",
    "   'capital-gain': {'int64List': {'value': ['2174']}}, \n",
    "   'education-num': {'int64List': {'value': ['13']}}, \n",
    "   'workclass': {'bytesList': {'value': ['IFN0YXRlLWdvdg==']}}, \n",
    "   'fnlwgt': {'int64List': {'value': ['77516']}}, \n",
    "   'capital-loss': {'int64List': {'value': ['0']}}  \n",
    "      }\n",
    "         }\n",
    "            }\n",
    "    \n",
    "'''    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f7d94",
   "metadata": {},
   "source": [
    "![title](TFX_comp_StatGenSchemaGenExampleVal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a89dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### StatisticsGen from Data Validation ###\n",
    "##########################################\n",
    "\n",
    "### Subect 1: To instantiate StatisticsGen with the ExampleGen ingested dataset\n",
    "\n",
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    examples = example_gen.outputs['examples'])\n",
    "\n",
    "# Execute the component\n",
    "context.run(statistics_gen)\n",
    "\n",
    "# Show the output statistics\n",
    "context.show(statistics_gen.outputs['statistics']) # practically this is similar to tfdv.generate_statistics_from_dataframe and tfdv.visualize_statistics\n",
    "\n",
    "\n",
    "######################################\n",
    "### SchemaGen from Data Validation ###\n",
    "######################################\n",
    "\n",
    "### Subject 1: Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
    "schema_gen = tfx.components.SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    ) # Similar role to tfdv.infer_schema and tfdv.display_schema\n",
    "\n",
    "\n",
    "# Run the component\n",
    "context.run(schema_gen)\n",
    "\n",
    "# Visualize the schema\n",
    "context.show(schema_gen.outputs['schema'])\n",
    "\n",
    "\n",
    "##############################################\n",
    "### Example Validator from Data Validation ###\n",
    "##############################################\n",
    "\n",
    "### Subject 1: Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n",
    "example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema']) # Similar role to tfdv.validate_statistics and tfdv.display_anomalies()\n",
    "\n",
    "# Run the component\n",
    "context.run(example_validator)\n",
    "\n",
    "# Visualize the results\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc9d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
